{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Spatial Analysis with [PySAL](https://pysal.org)\n",
    "\n",
    "Examples used here are based on the [excellent tutorials](http://darribas.org/materials.html) by Dani Arribas-Bel.\n",
    "\n",
    "### Let's start with the PySal *viz* component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "import libpysal \n",
    "from libpysal import weights\n",
    "from pysal.explore import esda\n",
    "from pysal.viz import mapclassify\n",
    "#from pysal.viz.splot.mapping import vba_choropleth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10] # change standard figure size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a [geopackage](http://www.geopackage.org) with districts in [Belo Horizonte](https://en.wikipedia.org/wiki/Belo_Horizonte), reproject it to [EPSG:3857](https://epsg.io/3857), and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = gpd.read_file('https://github.com/darribas/gds_ufmg19/raw/master/data/bh.gpkg').to_crs(epsg=3857)\n",
    "db.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn [color palettes](https://seaborn.pydata.org/tutorial/color_palettes.html)\n",
    "\n",
    "Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(sns.color_palette('viridis', 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divergent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(sns.color_palette('coolwarm', 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(sns.color_palette('Set2', 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning colors to values: Classification \n",
    "\n",
    "In the raster lecture, we already used a non-linear mapping from data values to colors. There are more elaborate ways to do this, though, particularly if we want to classify the data so that we can reduce the number of color values on the map to preserve readability. We'll go through a few ways to do this (giving you some cartography skills about [Choropleth Maps](https://onlinelibrary-wiley-com.zorac.aub.aau.dk/doi/abs/10.1002/9781118786352.wbieg0951) along the way), starting with\n",
    "\n",
    "**Equal intervals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.EqualInterval(db['Average Monthly Wage'], k=7)\n",
    "classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi.bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make ourselves a little function that will nicely show where the boundaries between the classes are when we want to compare the different classification methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotClassification(classi):\n",
    "    # Set up the figure\n",
    "    f, ax = plt.subplots(1, figsize=(9, 6))\n",
    "    # Plot the kernel density estimation (KDE)\n",
    "    sns.kdeplot(db['Average Monthly Wage'], shade=True)\n",
    "    # Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "    sns.rugplot(db['Average Monthly Wage'], alpha=0.5)\n",
    "    # Loop over each break point and plot a vertical red line\n",
    "    for cut in classi.bins:\n",
    "        plt.axvline(cut, color='red', linewidth=0.75)\n",
    "    # Title\n",
    "    ax.set_title(classi.name)\n",
    "    # Display image\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotClassification(classi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantiles**\n",
    "\n",
    "The equal intervals method splits up the data into, *ahem*, equal intervals. Quantiles instead arranges the class boundaries so that we have the same number of data points in each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.Quantiles(db['Average Monthly Wage'], k=7)\n",
    "classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotClassification(classi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Fisher-Jenks](https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization)** is a data clustering method designed to determine the \"best\" arrangement of values, such that natural breaks are identified and used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.FisherJenks(db['Average Monthly Wage'], k=7)\n",
    "classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotClassification(classi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boxplot** should sound familiar (if not, go back to the exploratory data analysis notebook). This method will use the same breaks use for the boxplot to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.BoxPlot(db['Average Monthly Wage'])\n",
    "classi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the classification relate to the box plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "f, axs = plt.subplots(2, figsize=(10, 10), gridspec_kw = {'height_ratios':[3, 1]})\n",
    "# Plot the kernel density estimation (KDE)\n",
    "sns.kdeplot(db['Average Monthly Wage'], shade=True, ax=axs[0])\n",
    "# Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "sns.rugplot(db['Average Monthly Wage'], alpha=0.5, ax=axs[0])\n",
    "# Loop over each break point and plot a vertical red line\n",
    "for cut in classi.bins:\n",
    "    axs[0].axvline(cut, color='red', linewidth=0.75)\n",
    "# Box-Plot\n",
    "axs[1].boxplot(db['Average Monthly Wage'], vert=False)\n",
    "# Set X axis manually\n",
    "axs[1].set_xlim(axs[0].get_xlim())\n",
    "# Title\n",
    "axs[0].set_title(classi.name)\n",
    "# Display image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some\n",
    "\n",
    "## Choropleth maps\n",
    "\n",
    "Use the classification methods to assign colors to features on the map.\n",
    "\n",
    "üèã Which of the following maps is the best? Which one is correct? Let's discuss during the Q&A!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for classification in ['equal_interval', 'quantiles', 'fisher_jenks']:\n",
    "    f, ax = plt.subplots(1, figsize=(14, 14))\n",
    "    db.plot(column='Average Monthly Wage', scheme=classification, ax=ax, legend=True)\n",
    "    ctx.add_basemap(ax, url=ctx.sources.ST_TONER_LITE)\n",
    "    ax.set_axis_off()\n",
    "    plt.axis('equal')\n",
    "    plt.title(classification)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Spatial Weights](http://darribas.org/gds_scipy16/ipynb_md/03_spatial_weights.html)\n",
    "\n",
    "Spatial weights are central components of many areas of spatial analysis. In general terms, for a spatial data set composed of *n* locations (points, areal units, network edges, etc.), the spatial weights matrix expresses the potential for interaction between observations at each pair *i,j* of locations. There is a rich variety of ways to specify the structure of these weights, and PySAL supports the creation, manipulation and analysis of spatial weights matrices across three different general types:\n",
    "\n",
    "- Contiguity Based Weights\n",
    "- Distance Based Weights\n",
    "- Kernel Weights\n",
    "\n",
    "Let's take a look at a lattice example with fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a regular 3x3 lattice and draw it here\n",
    "w = weights.lat2W(3, 3, rook=True)\n",
    "# Get points in a grid\n",
    "l = np.arange(3)\n",
    "xs, ys = np.meshgrid(l, l)\n",
    "# Set up store\n",
    "polys = []\n",
    "# Generate polygons\n",
    "for x, y in zip(xs.flatten(), ys.flatten()):\n",
    "    poly = Polygon([(x, y), (x+1, y), (x+1, y+1), (x, y+1)])\n",
    "    polys.append(poly)\n",
    "# Convert to GeoSeries\n",
    "polys = gpd.GeoSeries(polys)\n",
    "gdf = gpd.GeoDataFrame({'geometry': polys, \n",
    "                        'id': ['P-%s'%str(i).zfill(2) for i in range(len(polys))]})\n",
    "w.remap_ids(gdf['id'].values)\n",
    "# Annotate & Visualise\n",
    "ax = polys.plot(edgecolor='k', facecolor='w')\n",
    "[plt.text(x, y, t, \n",
    "          verticalalignment='center',\n",
    "          horizontalalignment='center') for x, y, t in zip(\n",
    "         [p.centroid.x for p in polys],\n",
    "         [p.centroid.y for p in polys],\n",
    "         [i for i in gdf['id']])]\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate a contiguity matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(w.full()[0], \n",
    "             index=gdf['id'],\n",
    "             columns=gdf['id'],\n",
    "            ).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world data: Belo Horizonte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_queen = weights.Queen.from_dataframe(db)\n",
    "pd.DataFrame(w_queen.full()[0], \n",
    "             index=db['CD_GEOCMU'],\n",
    "             columns=db['CD_GEOCMU'],\n",
    "            ).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Rook's case vs. Queen's case: \n",
    "\n",
    "- In Rook's case, only sees shared **edges** lead to a connection\n",
    "- In Queen's case, also shared **vertices** (points) lead to a connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct weights based on K Nearest Neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_k1 = weights.KNN.from_dataframe(db, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the difference between the two weights matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "db.plot(color='k', ax=axs[0], \n",
    "        edgecolor='w', linewidth=0.5)\n",
    "w_queen.plot(db, ax=axs[0], color='red')\n",
    "axs[0].set_title('Queen weights')\n",
    "\n",
    "db.plot(color='k', ax=axs[1], \n",
    "        edgecolor='w', linewidth=0.5)\n",
    "w_k1.plot(db, ax=axs[1], color='red')\n",
    "axs[1].set_title('KNN-1 weights')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out https://geographicdata.science/book/notebooks/06_spatial_autocorrelation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Moran's I](https://en.wikipedia.org/wiki/Moran%27s_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "db['Random Industry Diversity'] = db['Industry Diversity'].sample(frac=1).values\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran = esda.Moran(db['Industry Diversity'], w_queen)\n",
    "moran.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran.p_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran_shuffled = esda.Moran(db['Random Industry Diversity'], w_queen)\n",
    "moran_shuffled.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran_shuffled.p_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Pattern Analysis\n",
    "\n",
    "[This](https://mgimond.github.io/Spatial/point-pattern-analysis.html) is a really good read for a quick and dirty intro to PPA. Please also take a look at this chapter on [Point Sets and Distance Statistics](https://www.spatialanalysisonline.com/HTML/index.html?point_sets_and_distance_statis.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import libpysal as ps\n",
    "from pointpats import PointPattern, PoissonPointProcess, as_window, K, Kenv\n",
    "from pointpats import PoissonPointProcess as csr\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juv = gpd.read_file(ps.examples.get_path(\"juvenile.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juv.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn that into a point pattern:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juv_points = np.array([[x,y] for x, y in zip(juv['X'], juv['Y'])]) # get x,y coordinates for all the points\n",
    "pp_juv = PointPattern(juv_points)\n",
    "pp_juv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_juv.plot(window= True, title= \"Point pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean centre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pointpats.centrography as ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_juv = ctr.mean_center(pp_juv.points)\n",
    "mc_juv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_juv.plot()\n",
    "plt.plot(mc_juv[0], mc_juv[1], 'r^', label='Mean Center')\n",
    "plt.legend(numpoints=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard distance and summary circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdd = ctr.std_distance(pp_juv.points)\n",
    "stdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circle1=plt.Circle((mc_juv[0], mc_juv[1]),stdd,color='y')\n",
    "ax = pp_juv.plot(get_ax=True, title='Standard Distance Circle')\n",
    "ax.add_artist(circle1)\n",
    "plt.plot(mc_juv[0], mc_juv[1], 'r^', label='Mean Center')\n",
    "ax.set_aspect('equal')\n",
    "plt.legend(numpoints=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Deviational Ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx, sy, theta = ctr.ellipse(pp_juv.points)\n",
    "sx, sy, theta # size x, size y, rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_degree = np.degrees(theta) #need degree of rotation to plot the ellipse\n",
    "theta_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "from pylab import figure, show,rand\n",
    "\n",
    "fig = figure()\n",
    "e = Ellipse(xy=ctr.mean_center(pp_juv.points), width=sx*2, height=sy*2, angle=-theta_degree) #angle is rotation in degrees (anti-clockwise)\n",
    "ax = pp_juv.plot(get_ax=True, title='Standard Deviational Ellipse')\n",
    "ax.add_artist(e)\n",
    "e.set_clip_box(ax.bbox)\n",
    "e.set_facecolor([0.8,0,0])\n",
    "e.set_edgecolor([1,0,0])\n",
    "ax.set_xlim(0,100)\n",
    "ax.set_ylim(0,100)\n",
    "ax.set_aspect('equal')\n",
    "plt.plot(mc_juv[0], mc_juv[1], 'b^', label='Mean Center')\n",
    "plt.legend(numpoints=1)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_juv.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Quadrat statistic](https://pysal.org/notebooks/explore/pointpats/Quadrat_statistics.html)\n",
    "\n",
    "This is a method to place equally sized squares over the study area (randomly or exhaustively), count the number of points in each square, and compare it to a random point pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pointpats.quadrat_statistics as qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_r = qs.QStatistic(pp_juv,shape= \"rectangle\",nx = 3, ny = 3)\n",
    "q_r.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the frequency distribution significantly vary from CSR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_r.chi2 #chi-squared test statistic for the observed point pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_r.chi2_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 999 realizations of CSR and compare against those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_process = csr(pp_juv.window, pp_juv.n, 999, asPP=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_r_e = qs.QStatistic(pp_juv,shape= \"rectangle\",nx = 3, ny = 3, realizations = csr_process)\n",
    "q_r_e.chi2_r_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reject the null hypothesis that the juvenile point pattern is a result of CSR at the 95% confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [K Function](https://www.spatialanalysisonline.com/HTML/index.html?pairwise_distances.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realizations = PoissonPointProcess(pp_juv.window, pp_juv.n, 100, asPP=True)\n",
    "kenv = Kenv(pp_juv, intervals=20, realizations=realizations)\n",
    "kenv.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ _Don't forget Kernel Density Estimation!_ ‚Äì¬†also a nice and quick way to quickly identify clusters visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
